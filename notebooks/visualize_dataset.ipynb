{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd08385ea9c1b89c046143ed892d3a6417099bb117058907eb25ccd44462e08e108",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadImages\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.torch_utils import select_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\cart_scraped_42.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\cart_scraped_45.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\gas_cylinder_scraped_102.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\gas_cylinder_scraped_47.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\gas_cylinder_scraped_91.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\table_scraped_224.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\table_scraped_251.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\table_scraped_82.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\tissue_scraped_130.jpg\n",
      "c:\\GitRepos\\yolov5_workspace\\oidv6_data_converted_11classes\\images\\train\\tissue_scraped_131.jpg\n",
      "43076\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path.cwd().parents[1] / \"oidv6_data_converted_11classes\"\n",
    "count = 0\n",
    "for path in (data_dir / \"images\" / \"train\").glob(\"*.jpg\"):\n",
    "    if (data_dir / \"labels\" / \"train\" / f\"{path.stem}.txt\").exists():\n",
    "        with open(data_dir / \"labels\" / \"train\" / f\"{path.stem}.txt\", \"r\") as f:\n",
    "            l = [x.split() for x in f.read().strip().splitlines()]\n",
    "            l = np.array(l, dtype=np.float32)\n",
    "        if not len(l):\n",
    "            os.remove(path)\n",
    "            os.remove(data_dir / \"labels\" / \"train\" / f\"{path.stem}.txt\")\n",
    "            print(path)\n",
    "        else:\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device(\"\")\n",
    "source = Path.cwd().parent / \"data\" / \"images\"\n",
    "weights = Path.cwd().parent / \"weights\" / \"yolov5l_freeze_11classes_scraped.pt\"\n",
    "imgsz = 640\n",
    "\n",
    "conf_thres = 0.15\n",
    "iou_thres = 0.45\n",
    "classes = None\n",
    "agnostic_nms = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "names = model.module.names if hasattr(model, \"module\") else model.names  # get class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LoadImages(source, img_size=imgsz, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "image 1/24 c:\\GitRepos\\yolov5_workspace\\yolov5\\data\\images\\burner-cart-wok-1.jpg: tensor([4.39330e+00, 6.24763e+00, 1.14381e+01, 1.29285e+01, 7.94411e-06, 1.03331e-01, 3.34693e-02, 4.76894e-02, 4.16172e-02, 1.13475e-02, 1.18452e-02, 2.34487e-01, 1.95195e-02, 1.69694e-02, 5.56443e-02, 1.47641e-02])\n",
      "[tensor([[4.05101e+02, 2.70091e+02, 5.60650e+02, 3.35862e+02, 7.25105e-01, 4.00000e+00],\n",
      "        [1.11106e+02, 2.46666e+02, 4.99862e+02, 4.82934e+02, 6.19453e-01, 1.00000e+01],\n",
      "        [4.38403e+02, 3.23211e+02, 5.30273e+02, 4.12509e+02, 5.38645e-01, 9.00000e+00],\n",
      "        [1.23770e+02, 3.00477e+02, 1.54705e+02, 3.15651e+02, 3.57982e-01, 1.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "for path, img, im0s, vid_cap in dataset:\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    pred = model(img, augment=False)[0]\n",
    "    pred_nms = non_max_suppression(pred, 0.001, 0.6, classes, agnostic_nms)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}